{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "B-A5R0LTffss",
    "outputId": "d43bc701-db54-4f7d-d9f9-eec89bef6626"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "# Install necessary packages required for the notebook.\n",
    "\n",
    "!pip install nba_api pandas numpy scikit-learn lightgbm xgboost seaborn matplotlib ipywidgets optuna joblib shap requests tensorflow fastapi uvicorn nest_asyncio pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sH4e9qOdfoeL",
    "outputId": "3ae9ab08-1477-492d-fc0d-54ea54b00dee"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries and Set Up Environment\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warning messages\n",
    "\n",
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Import data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import datetime utilities\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# Import deep learning libraries\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Import Optuna for hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "# Import SHAP for model interpretation\n",
    "import shap\n",
    "\n",
    "# Import requests and retry adapters\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Import NBA API libraries\n",
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "from nba_api.stats.static import teams\n",
    "\n",
    "# Import FastAPI and Uvicorn for API development\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Import ngrok for exposing local server (optional)\n",
    "if IN_COLAB:\n",
    "    from pyngrok import ngrok\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    ")\n",
    "logging.info(\"Logging configured successfully\")\n",
    "\n",
    "# Mount Google Drive if running in Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# Set up base directories using pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"/content/drive/MyDrive/nba_models\") if IN_COLAB else Path(\"nba_models\")\n",
    "cache_dir = base_dir / \"cache\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)  # Create directories if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "oXzEfNnefsOe"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Configuration Class\n",
    "# This class stores all the configuration parameters used throughout the notebook.\n",
    "\n",
    "class NotebookConfig:\n",
    "    # Model parameters for XGBoost and LightGBM\n",
    "    XGB_PARAMS = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': 42,\n",
    "        'n_estimators': 100,\n",
    "        'eval_metric': 'logloss',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1\n",
    "    }\n",
    "\n",
    "    LGB_PARAMS = {\n",
    "        'objective': 'binary',\n",
    "        'random_state': 42,\n",
    "        'n_estimators': 100,\n",
    "        'metric': 'binary_logloss',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.1\n",
    "    }\n",
    "\n",
    "    # Training parameters\n",
    "    VALIDATION_WINDOW = 60  # Days\n",
    "    RETRAIN_FREQUENCY = 7   # Days\n",
    "    MIN_TRAINING_SAMPLES = 1000\n",
    "    ROLLING_WINDOWS = [5, 10, 20]  # For rolling statistics\n",
    "    HEAD_TO_HEAD_WINDOW = 10  # Games\n",
    "\n",
    "    # Data parameters\n",
    "    START_SEASON = '2015-16'\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    # API Configuration for fetching odds data\n",
    "    ODDS_API_KEY = '55bc8ab276ec29e4a474114a4eccb463'  # Replace with your actual API key\n",
    "    ODDS_API_URL = 'https://api.the-odds-api.com/v4/sports/basketball_nba/odds'\n",
    "    ODDS_API_HISTORICAL_URL = 'https://api.the-odds-api.com/v4/sports/basketball_nba/odds-history'\n",
    "    REGIONS = 'us'\n",
    "    MARKETS = 'h2h'\n",
    "    ODDS_FORMAT = 'american'\n",
    "    BOOKMAKERS = 'fanduel'\n",
    "    BATCH_SIZE = 10\n",
    "\n",
    "    # Paths for models and cache directories\n",
    "    MODELS_DIR = base_dir\n",
    "    CACHE_DIR = cache_dir\n",
    "\n",
    "    # Betting parameters\n",
    "    MIN_KELLY_FRACTION = 0.1\n",
    "    MAX_KELLY_FRACTION = 0.5\n",
    "    MIN_CONFIDENCE = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTkdwm5xfz3j",
    "outputId": "b377db3a-d239-4804-fa9b-ab22d57f93ba"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Data Manager Class\n",
    "# This class handles data saving, loading, and cache validation.\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self):\n",
    "        self.config = NotebookConfig()\n",
    "\n",
    "    def get_cache_path(self, filename):\n",
    "        \"\"\"Get the full path to the cache file.\"\"\"\n",
    "        return self.config.CACHE_DIR / filename\n",
    "\n",
    "    def save_data(self, data, filename):\n",
    "        \"\"\"Save data to a cache file using pickle.\"\"\"\n",
    "        cache_path = self.get_cache_path(filename)\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        logging.info(f\"Data saved to {cache_path}\")\n",
    "\n",
    "    def load_data(self, filename):\n",
    "        \"\"\"Load data from a cache file.\"\"\"\n",
    "        cache_path = self.get_cache_path(filename)\n",
    "        if cache_path.exists():\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            logging.info(f\"Data loaded from {cache_path}\")\n",
    "            return data\n",
    "        return None\n",
    "\n",
    "    def is_cache_fresh(self, filename, max_age_hours=24):\n",
    "        \"\"\"Check if the cache file is fresh within the specified hours.\"\"\"\n",
    "        cache_path = self.get_cache_path(filename)\n",
    "        try:\n",
    "            if not cache_path.exists():\n",
    "                return False\n",
    "            file_time = datetime.fromtimestamp(cache_path.stat().st_mtime)\n",
    "            return (datetime.now() - file_time).total_seconds() < max_age_hours * 3600\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error checking cache freshness: {e}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "oddBHX7Cf3-D"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Data Fetching Functions\n",
    "# These functions handle data fetching from the NBA API with retry logic.\n",
    "\n",
    "def fetch_nba_data_with_retry():\n",
    "    \"\"\"Fetch NBA game data with retry logic.\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Fetching all NBA game data...\")\n",
    "        gamefinder = leaguegamefinder.LeagueGameFinder()\n",
    "        all_games = gamefinder.get_data_frames()[0]\n",
    "        logging.info(f\"Fetched {len(all_games)} games.\")\n",
    "        return all_games\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching NBA data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def fetch_or_load_data():\n",
    "    \"\"\"Fetch data from API or load from cache.\"\"\"\n",
    "    data_manager = DataManager()\n",
    "    cache_filename = 'nba_game_data.pkl'\n",
    "\n",
    "    # Check if cached data is fresh\n",
    "    if data_manager.is_cache_fresh(cache_filename):\n",
    "        cached_data = data_manager.load_data(cache_filename)\n",
    "        if cached_data is not None:\n",
    "            # Ensure GAME_DATE is in datetime format\n",
    "            cached_data['GAME_DATE'] = pd.to_datetime(cached_data['GAME_DATE'])\n",
    "\n",
    "            # Check if new data is available\n",
    "            latest_game_date = cached_data['GAME_DATE'].max()\n",
    "\n",
    "            # Calculate difference in days\n",
    "            if (datetime.now() - latest_game_date).days < 1:\n",
    "                logging.info(\"Cache is fresh, loading data from cache.\")\n",
    "                return cached_data\n",
    "            else:\n",
    "                logging.info(\"Cache is stale, fetching new data.\")\n",
    "        else:\n",
    "            logging.info(\"No cached data found, fetching fresh data.\")\n",
    "\n",
    "    # Fetch new data if cache is unavailable or stale\n",
    "    game_data = fetch_nba_data_with_retry()\n",
    "\n",
    "    # Check if data was fetched successfully before saving\n",
    "    if game_data is not None and not game_data.empty:\n",
    "        data_manager.save_data(game_data, cache_filename)\n",
    "        logging.info(f\"New data fetched and saved to cache with {len(game_data)} games.\")\n",
    "    else:\n",
    "        logging.error(\"Failed to fetch new game data; not saving to cache.\")\n",
    "\n",
    "    return game_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "PRy8fYUqf96A",
    "outputId": "404d8f45-2c62-4c46-df9b-0d05f50d8a5a"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Data Processor Class\n",
    "# This class handles data processing and feature engineering.\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.config = NotebookConfig()\n",
    "        self.date_odds_cache = {}  # Cache odds data by date\n",
    "        self.batch_odds_data = {}  # Temporary store for batch processing\n",
    "\n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Prepare features for model training/prediction.\"\"\"\n",
    "        try:\n",
    "            # Create a copy to avoid modifying original\n",
    "            features = df.copy()\n",
    "\n",
    "            # Drop non-feature columns\n",
    "            drop_columns = [\n",
    "                'TEAM_NAME', 'MATCHUP', 'WL', 'TEAM_ABBREVIATION',\n",
    "                'OPPONENT_ABBREV', 'HOME_TEAM', 'AWAY_TEAM'\n",
    "            ]\n",
    "            features = features.drop([col for col in drop_columns if col in features.columns], axis=1)\n",
    "\n",
    "            # Convert categorical variables to numeric\n",
    "            categorical_columns = ['TEAM_ID', 'OPPONENT_TEAM_ID', 'HOME_TEAM_ID', 'AWAY_TEAM_ID']\n",
    "            for col in categorical_columns:\n",
    "                if col in features.columns:\n",
    "                    features[col] = features[col].astype('category').cat.codes\n",
    "\n",
    "            # Create game importance features\n",
    "            features['is_division_game'] = (features['TEAM_ID'] // 10 == features['OPPONENT_TEAM_ID'] // 10).astype(int)\n",
    "            features['month_number'] = pd.to_datetime(features['GAME_DATE']).dt.month\n",
    "            features['is_playoff_month'] = (features['month_number'] >= 4).astype(int)\n",
    "\n",
    "            # Handle missing values\n",
    "            numeric_columns = features.select_dtypes(include=['float64', 'int64']).columns\n",
    "            for col in numeric_columns:\n",
    "                features[col] = features[col].fillna(features[col].mean())\n",
    "\n",
    "            # Create interaction features\n",
    "            features['pts_per_min'] = features['PTS'] / features['MIN']\n",
    "            features['ast_to_tov'] = features['AST'] / (features['TOV'] + 1)  # Add 1 to avoid division by zero\n",
    "            features['fg_efficiency'] = features['FG_PCT'] * features['FGA']\n",
    "\n",
    "            # Create relative performance metrics\n",
    "            for stat in ['PTS', 'REB', 'AST']:\n",
    "                if f'{stat}_avg_10' in features.columns:\n",
    "                    features[f'{stat}_rel_performance'] = features[stat] / features[f'{stat}_avg_10']\n",
    "\n",
    "            # Normalize certain features\n",
    "            features['normalized_plus_minus'] = features['PLUS_MINUS'] / features['MIN']\n",
    "            features['usage_rate'] = (features['FGA'] + 0.44 * features['FTA'] + features['TOV']) / features['MIN']\n",
    "\n",
    "            # Sort features by date for time-series consistency\n",
    "            if 'GAME_DATE' in features.columns:\n",
    "                features = features.sort_values('GAME_DATE')\n",
    "\n",
    "            # Remove any remaining non-numeric columns except GAME_DATE and WIN\n",
    "            non_numeric_cols = features.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "            keep_cols = ['GAME_DATE', 'WIN']\n",
    "            drop_cols = [col for col in non_numeric_cols if col not in keep_cols]\n",
    "            features = features.drop(drop_cols, axis=1)\n",
    "\n",
    "            return features\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in prepare_features: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_game_data(self, df):\n",
    "        \"\"\"Preprocess the raw game data with improved NaN handling.\"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Initial data shape: {df.shape}\")\n",
    "            logging.info(f\"Initial columns: {df.columns}\")\n",
    "\n",
    "            df = df.copy()\n",
    "\n",
    "            # Create WIN column from WL\n",
    "            if 'WL' in df.columns:\n",
    "                df['WIN'] = df['WL'].apply(lambda x: 1 if x == 'W' else 0)\n",
    "                logging.info(\"Created WIN column from WL\")\n",
    "            else:\n",
    "                logging.error(\"WL column not found in data\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Convert GAME_DATE to datetime\n",
    "            df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'])\n",
    "            logging.info(f\"After date conversion shape: {df.shape}\")\n",
    "\n",
    "            # Sort data\n",
    "            df = df.sort_values(['TEAM_ID', 'GAME_DATE'])\n",
    "\n",
    "            # Map team abbreviations\n",
    "            from nba_api.stats.static import teams\n",
    "            nba_teams = teams.get_teams()\n",
    "            team_abbrev_to_id = {team['abbreviation']: team['id'] for team in nba_teams}\n",
    "            team_id_to_abbrev = {team['id']: team['abbreviation'] for team in nba_teams}\n",
    "\n",
    "            df['TEAM_ABBREVIATION'] = df['TEAM_ID'].map(team_id_to_abbrev)\n",
    "            logging.info(f\"After team abbreviation mapping shape: {df.shape}\")\n",
    "\n",
    "            # Handle home/away determination\n",
    "            df['HOME_AWAY'] = df['MATCHUP'].apply(lambda x: 'HOME' if 'vs.' in x else 'AWAY')\n",
    "            df['OPPONENT_ABBREV'] = df['MATCHUP'].apply(lambda x: x.split(' ')[-1])\n",
    "\n",
    "            # Create HOME_TEAM and AWAY_TEAM columns\n",
    "            def get_home_team(row):\n",
    "                return row['TEAM_ABBREVIATION'] if row['HOME_AWAY'] == 'HOME' else row['OPPONENT_ABBREV']\n",
    "\n",
    "            def get_away_team(row):\n",
    "                return row['TEAM_ABBREVIATION'] if row['HOME_AWAY'] == 'AWAY' else row['OPPONENT_ABBREV']\n",
    "\n",
    "            df['HOME_TEAM'] = df.apply(get_home_team, axis=1)\n",
    "            df['AWAY_TEAM'] = df.apply(get_away_team, axis=1)\n",
    "\n",
    "            # Map team IDs\n",
    "            df['OPPONENT_TEAM_ID'] = df['OPPONENT_ABBREV'].map(team_abbrev_to_id)\n",
    "            df['HOME_TEAM_ID'] = df['HOME_TEAM'].map(team_abbrev_to_id)\n",
    "            df['AWAY_TEAM_ID'] = df['AWAY_TEAM'].map(team_abbrev_to_id)\n",
    "\n",
    "            # Handle missing opponent team IDs\n",
    "            missing_opponents = df[df['OPPONENT_TEAM_ID'].isna()]['OPPONENT_ABBREV'].unique()\n",
    "            if len(missing_opponents) > 0:\n",
    "                logging.warning(f\"Missing opponent team IDs for: {missing_opponents}\")\n",
    "\n",
    "            # Drop rows with missing opponent team IDs\n",
    "            df = df.dropna(subset=['OPPONENT_TEAM_ID'])\n",
    "            logging.info(f\"After dropping missing opponents shape: {df.shape}\")\n",
    "\n",
    "            # Fill missing values in numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "            for col in numeric_cols:\n",
    "                df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "            # Convert IDs to integers with proper NaN handling\n",
    "            id_columns = ['TEAM_ID', 'OPPONENT_TEAM_ID', 'HOME_TEAM_ID', 'AWAY_TEAM_ID']\n",
    "            for col in id_columns:\n",
    "                df[col] = df[col].astype('float').fillna(-1).astype(int)\n",
    "\n",
    "            # Add features\n",
    "            logging.info(\"Adding fatigue features...\")\n",
    "            df = self.add_fatigue_features(df)\n",
    "\n",
    "            logging.info(\"Adding advanced features...\")\n",
    "            df = self.add_advanced_features(df)\n",
    "\n",
    "            logging.info(\"Adding rolling stats...\")\n",
    "            for window in self.config.ROLLING_WINDOWS:\n",
    "                df = self.add_rolling_stats(df, window)\n",
    "\n",
    "            logging.info(\"Adding head-to-head features...\")\n",
    "            df = self.add_head_to_head_features(df)\n",
    "\n",
    "            logging.info(\"Adding streak features...\")\n",
    "            df = self.add_streak_features(df)\n",
    "\n",
    "            logging.info(\"Adding time-based features...\")\n",
    "            df = self.add_time_based_features(df)\n",
    "\n",
    "            # Final NaN check and handling\n",
    "            df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "            logging.info(f\"Final shape: {df.shape}\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in preprocess_game_data: {str(e)}\")\n",
    "            import traceback\n",
    "            logging.error(traceback.format_exc())\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def add_advanced_features(self, df):\n",
    "        \"\"\"Add advanced statistical features.\"\"\"\n",
    "        try:\n",
    "            # Create a boolean for home games\n",
    "            df['is_home_game'] = df['HOME_AWAY'] == 'HOME'\n",
    "\n",
    "            # Create separate DataFrames for home and away games\n",
    "            home_games = df[df['is_home_game']].copy()\n",
    "            away_games = df[~df['is_home_game']].copy()\n",
    "\n",
    "            # Initialize columns with zeros\n",
    "            df['home_win_rate'] = 0.0\n",
    "            df['away_win_rate'] = 0.0\n",
    "\n",
    "            # Calculate win rates separately for home and away games\n",
    "            for team_id in df['TEAM_ID'].unique():\n",
    "                # Home win rate\n",
    "                team_home = home_games[home_games['TEAM_ID'] == team_id]\n",
    "                if not team_home.empty:\n",
    "                    win_rate = team_home['WIN'].rolling(10, min_periods=1).mean()\n",
    "                    df.loc[team_home.index, 'home_win_rate'] = win_rate\n",
    "\n",
    "                # Away win rate\n",
    "                team_away = away_games[away_games['TEAM_ID'] == team_id]\n",
    "                if not team_away.empty:\n",
    "                    win_rate = team_away['WIN'].rolling(10, min_periods=1).mean()\n",
    "                    df.loc[team_away.index, 'away_win_rate'] = win_rate\n",
    "\n",
    "            # Fill any remaining NaN values\n",
    "            df['home_win_rate'] = df['home_win_rate'].fillna(0.5)\n",
    "            df['away_win_rate'] = df['away_win_rate'].fillna(0.5)\n",
    "\n",
    "            # Difference between home and away win rates\n",
    "            df['home_road_diff'] = df['home_win_rate'] - df['away_win_rate']\n",
    "\n",
    "            # Head-to-head performance with minimum periods=1\n",
    "            df['h2h_wins'] = df.groupby(['TEAM_ID', 'OPPONENT_TEAM_ID'])['WIN'].transform(\n",
    "                lambda x: x.rolling(window=5, min_periods=1).mean().fillna(0.5)\n",
    "            )\n",
    "\n",
    "            # Recent form and trends with minimum periods=1\n",
    "            df['recent_form'] = df.groupby('TEAM_ID')['WIN'].transform(\n",
    "                lambda x: x.rolling(window=10, min_periods=1).mean().fillna(0.5)\n",
    "            )\n",
    "\n",
    "            df['pts_diff_trend'] = df.groupby('TEAM_ID')['PLUS_MINUS'].transform(\n",
    "                lambda x: x.rolling(window=5, min_periods=1).mean().fillna(0)\n",
    "            )\n",
    "\n",
    "            # Offensive and defensive ratings with minimum periods=1\n",
    "            df['off_rating'] = df.groupby('TEAM_ID')['PTS'].transform(\n",
    "                lambda x: x.rolling(window=5, min_periods=1).mean().fillna(x.mean())\n",
    "            )\n",
    "\n",
    "            df['def_rating'] = df.groupby('OPPONENT_TEAM_ID')['PTS'].transform(\n",
    "                lambda x: x.rolling(window=5, min_periods=1).mean().fillna(x.mean())\n",
    "            )\n",
    "\n",
    "            # Scoring consistency with minimum periods=1\n",
    "            df['scoring_consistency'] = df.groupby('TEAM_ID')['PTS'].transform(\n",
    "                lambda x: x.rolling(window=5, min_periods=1).std().fillna(0)\n",
    "            )\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in add_advanced_features: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "\n",
    "    def add_rolling_stats(self, df, window):\n",
    "        \"\"\"Add rolling statistics for a given window size.\"\"\"\n",
    "        try:\n",
    "            stats_columns = ['PTS', 'REB', 'AST', 'PLUS_MINUS', 'FG_PCT', 'FT_PCT', 'FG3_PCT']\n",
    "\n",
    "            for col in stats_columns:\n",
    "                # Calculate rolling statistics with min_periods=1\n",
    "                df[f'{col}_avg_{window}'] = df.groupby('TEAM_ID')[col].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=1).mean().fillna(x.mean())\n",
    "                )\n",
    "\n",
    "                df[f'{col}_std_{window}'] = df.groupby('TEAM_ID')[col].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=1).std().fillna(0)\n",
    "                )\n",
    "\n",
    "                df[f'{col}_max_{window}'] = df.groupby('TEAM_ID')[col].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=1).max().fillna(x.mean())\n",
    "                )\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in add_rolling_stats: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "\n",
    "    def add_head_to_head_features(self, df):\n",
    "        \"\"\"Add head-to-head matchup features.\"\"\"\n",
    "        try:\n",
    "            # Calculate h2h win rate with minimum periods=1 and fill NaN with 0.5\n",
    "            df['h2h_win_rate'] = df.groupby(['TEAM_ID', 'OPPONENT_TEAM_ID'])['WIN'].transform(\n",
    "                lambda x: x.rolling(window=self.config.HEAD_TO_HEAD_WINDOW, min_periods=1).mean().fillna(0.5)\n",
    "            )\n",
    "\n",
    "            # Calculate point differential with minimum periods=1 and fill NaN with 0\n",
    "            df['h2h_point_diff'] = df.groupby(['TEAM_ID', 'OPPONENT_TEAM_ID'])['PLUS_MINUS'].transform(\n",
    "                lambda x: x.rolling(window=self.config.HEAD_TO_HEAD_WINDOW, min_periods=1).mean().fillna(0)\n",
    "            )\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in add_head_to_head_features: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "    def add_fatigue_features(self, df):\n",
    "        \"\"\"Add fatigue and travel-related features.\"\"\"\n",
    "        # Ensure dates are in datetime format (redundant safety check)\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['GAME_DATE']):\n",
    "            df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'])\n",
    "\n",
    "        # Sort by team and date\n",
    "        df = df.sort_values(['TEAM_ID', 'GAME_DATE']).copy()\n",
    "\n",
    "        # Calculate days between games\n",
    "        df['days_since_last'] = df.groupby('TEAM_ID')['GAME_DATE'].diff().dt.days.fillna(0)\n",
    "\n",
    "        # Calculate games in last 7 days using vectorized operations\n",
    "        def calculate_rolling_games(group):\n",
    "            # Create a Series with the game dates\n",
    "            dates = pd.Series(group['GAME_DATE'])\n",
    "            counts = []\n",
    "\n",
    "            for current_date in dates:\n",
    "                # Count games in previous 7 days (excluding current game)\n",
    "                count = ((dates < current_date) &\n",
    "                        (dates >= current_date - pd.Timedelta(days=7))).sum()\n",
    "                counts.append(count)\n",
    "\n",
    "            return counts\n",
    "\n",
    "        # Apply the calculation to each team\n",
    "        df['games_last_7d'] = df.groupby('TEAM_ID').apply(\n",
    "            calculate_rolling_games\n",
    "        ).explode().values\n",
    "\n",
    "        # Calculate back-to-back games\n",
    "        df['is_back_to_back'] = (df['days_since_last'] == 1).astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def add_streak_features(self, df):\n",
    "        \"\"\"Add streak-related features.\"\"\"\n",
    "        try:\n",
    "            def get_streak(series):\n",
    "                streak = 0\n",
    "                streaks = []\n",
    "                for val in series:\n",
    "                    if val == 1:\n",
    "                        streak = streak + 1 if streak >= 0 else 1\n",
    "                    else:\n",
    "                        streak = streak - 1 if streak <= 0 else -1\n",
    "                    streaks.append(streak)\n",
    "                return pd.Series(streaks, index=series.index)\n",
    "\n",
    "            # Calculate streaks\n",
    "            df['streak'] = df.groupby('TEAM_ID')['WIN'].transform(\n",
    "                lambda x: get_streak(x)\n",
    "            )\n",
    "\n",
    "            # Calculate momentum with minimum periods=1\n",
    "            weights = np.array([0.35, 0.25, 0.20, 0.15, 0.05])\n",
    "            def weighted_momentum(series):\n",
    "                return series.rolling(5, min_periods=1).apply(\n",
    "                    lambda x: np.sum(weights[-len(x):] * x) / np.sum(weights[-len(x):])\n",
    "                ).fillna(0.5)\n",
    "\n",
    "            df['momentum'] = df.groupby('TEAM_ID')['WIN'].transform(weighted_momentum)\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in add_streak_features: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "    def add_time_based_features(self, df):\n",
    "        \"\"\"Add time-based features such as season phase and rest days.\"\"\"\n",
    "        try:\n",
    "            logging.info(\"Starting add_time_based_features\")\n",
    "            logging.info(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "            # Season phase (regular season or playoffs)\n",
    "            df['month'] = df['GAME_DATE'].dt.month\n",
    "            df['season_phase'] = df['month'].apply(\n",
    "                lambda x: 'playoffs' if x >= 4 else 'regular'\n",
    "            )\n",
    "\n",
    "            # Encode season phase\n",
    "            le = LabelEncoder()\n",
    "            df['season_phase'] = le.fit_transform(df['season_phase'])\n",
    "\n",
    "            # Rest days between games\n",
    "            df['rest_days'] = df.groupby('TEAM_ID')['GAME_DATE'].diff().dt.days.fillna(0)\n",
    "\n",
    "            logging.info(f\"Final shape after time-based features: {df.shape}\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in add_time_based_features: {str(e)}\")\n",
    "            return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true,
    "id": "vQQj6vsigF3H"
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.config = NotebookConfig()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test, model_name):\n",
    "        \"\"\"Evaluate model with proper error handling.\"\"\"\n",
    "        try:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            else:\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred = (y_pred > 0.5).astype(int)\n",
    "                y_pred_proba = y_pred\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "\n",
    "            logging.info(f\"{model_name} Evaluation:\")\n",
    "            logging.info(f\"  Accuracy: {acc:.4f}\")\n",
    "            logging.info(f\"  F1 Score: {f1:.4f}\")\n",
    "            logging.info(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "            logging.info(f\"  Precision: {precision:.4f}\")\n",
    "            logging.info(f\"  Recall: {recall:.4f}\")\n",
    "\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            logging.info(f\"  Confusion Matrix:\")\n",
    "            logging.info(f\"    TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "            logging.info(f\"    FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error evaluating {model_name}: {str(e)}\")\n",
    "\n",
    "    def preprocess_features(self, X_train, X_val, X_test):\n",
    "        \"\"\"Preprocess features with improved scaling and imputation.\"\"\"\n",
    "        # Convert to DataFrame if not already\n",
    "        if not isinstance(X_train, pd.DataFrame):\n",
    "            X_train = pd.DataFrame(X_train)\n",
    "            X_val = pd.DataFrame(X_val)\n",
    "            X_test = pd.DataFrame(X_test)\n",
    "\n",
    "        feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "        X_train.columns = feature_names\n",
    "        X_val.columns = feature_names\n",
    "        X_test.columns = feature_names\n",
    "\n",
    "        # First handle missing values\n",
    "        X_train_imputed = self.imputer.fit_transform(X_train)\n",
    "        X_val_imputed = self.imputer.transform(X_val)\n",
    "        X_test_imputed = self.imputer.transform(X_test)\n",
    "\n",
    "        # Then scale the data\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train_imputed)\n",
    "        X_val_scaled = self.scaler.transform(X_val_imputed)\n",
    "        X_test_scaled = self.scaler.transform(X_test_imputed)\n",
    "\n",
    "        # Convert back to DataFrame\n",
    "        X_train_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "        X_val_df = pd.DataFrame(X_val_scaled, columns=feature_names)\n",
    "        X_test_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "\n",
    "        return X_train_df, X_val_df, X_test_df\n",
    "\n",
    "    def train_baseline_model(self, X_train, y_train):\n",
    "        \"\"\"Train a more robust baseline model with balanced class weights.\"\"\"\n",
    "        model = LogisticRegression(\n",
    "            class_weight='balanced',\n",
    "            penalty='l1',\n",
    "            solver='liblinear',\n",
    "            random_state=self.config.RANDOM_SEED,\n",
    "            max_iter=1000\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def optimize_xgboost_params(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Optimize XGBoost with improved parameters.\"\"\"\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "                'gamma': trial.suggest_float('gamma', 0, 2),\n",
    "                'scale_pos_weight': sum(y_train == 0) / sum(y_train == 1),\n",
    "                'objective': 'binary:logistic',\n",
    "                'tree_method': 'exact',\n",
    "                'random_state': self.config.RANDOM_SEED,\n",
    "                'missing': np.nan  # Explicitly handle missing values\n",
    "            }\n",
    "\n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict_proba(X_val)[:, 1]\n",
    "            return roc_auc_score(y_val, y_pred)\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "\n",
    "        return study.best_params\n",
    "\n",
    "    def optimize_lightgbm_params(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Optimize LightGBM with improved parameters.\"\"\"\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 8, 32),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'deterministic': True,\n",
    "                'force_row_wise': True,\n",
    "                'min_data_in_leaf': 20,\n",
    "                'scale_pos_weight': sum(y_train == 0) / sum(y_train == 1),\n",
    "                'random_state': self.config.RANDOM_SEED\n",
    "            }\n",
    "\n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict_proba(X_val)[:, 1]\n",
    "            return roc_auc_score(y_val, y_pred)\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "\n",
    "        return study.best_params\n",
    "\n",
    "    def create_neural_network(self, input_dim):\n",
    "        \"\"\"Create a neural network with proper input dimension handling.\"\"\"\n",
    "        try:\n",
    "            if input_dim <= 0:\n",
    "                raise ValueError(f\"Invalid input dimension: {input_dim}\")\n",
    "\n",
    "            model = Sequential([\n",
    "                Dense(64, input_shape=(input_dim,), activation='relu',\n",
    "                      kernel_regularizer=l2(0.01)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.3),\n",
    "\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.2),\n",
    "\n",
    "                Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.1),\n",
    "\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=0.001),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating neural network: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train_models(self, X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "        \"\"\"Train all models with improved error handling and class weight calculation.\"\"\"\n",
    "        try:\n",
    "            logging.info(\"Preprocessing features...\")\n",
    "\n",
    "            # Validate input data\n",
    "            if X_train.shape[0] == 0 or X_train.shape[1] == 0:\n",
    "                raise ValueError(f\"Invalid training data shape: {X_train.shape}\")\n",
    "\n",
    "            # Preprocess the data\n",
    "            X_train_processed, X_val_processed, X_test_processed = self.preprocess_features(\n",
    "                X_train, X_val, X_test\n",
    "            )\n",
    "\n",
    "            # Convert targets to proper format and numpy arrays\n",
    "            y_train = y_train.astype(int).values\n",
    "            y_val = y_val.astype(int).values\n",
    "            y_test = y_test.astype(int).values\n",
    "\n",
    "            # Initialize models list\n",
    "            trained_models = []\n",
    "\n",
    "            # Train baseline model\n",
    "            logging.info(\"Training baseline Logistic Regression model...\")\n",
    "            baseline_model = self.train_baseline_model(X_train_processed, y_train)\n",
    "            trained_models.append(baseline_model)\n",
    "            self.evaluate_model(baseline_model, X_test_processed, y_test, \"Baseline Logistic Regression\")\n",
    "\n",
    "            # Train XGBoost\n",
    "            logging.info(\"Training XGBoost model...\")\n",
    "            xgb_params = self.optimize_xgboost_params(X_train_processed, y_train, X_val_processed, y_val)\n",
    "            xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "            xgb_model.fit(X_train_processed, y_train)\n",
    "            trained_models.append(xgb_model)\n",
    "            self.evaluate_model(xgb_model, X_test_processed, y_test, \"XGBoost\")\n",
    "\n",
    "            # Train LightGBM\n",
    "            logging.info(\"Training LightGBM model...\")\n",
    "            lgb_params = self.optimize_lightgbm_params(X_train_processed, y_train, X_val_processed, y_val)\n",
    "            lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "            lgb_model.fit(X_train_processed, y_train)\n",
    "            trained_models.append(lgb_model)\n",
    "            self.evaluate_model(lgb_model, X_test_processed, y_test, \"LightGBM\")\n",
    "\n",
    "            # Train Neural Network\n",
    "            logging.info(\"Training Neural Network model...\")\n",
    "            input_dim = X_train_processed.shape[1]\n",
    "            if input_dim <= 0:\n",
    "                raise ValueError(f\"Invalid input dimension for neural network: {input_dim}\")\n",
    "\n",
    "            logging.info(f\"Creating neural network with input dimension: {input_dim}\")\n",
    "            nn_model = self.create_neural_network(input_dim)\n",
    "\n",
    "            # Calculate class weights properly using numpy\n",
    "            n_negative = np.sum(y_train == 0)\n",
    "            n_positive = np.sum(y_train == 1)\n",
    "            class_weights = {\n",
    "                0: 1.0,\n",
    "                1: (n_negative / n_positive) if n_positive > 0 else 1.0\n",
    "            }\n",
    "\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001)\n",
    "            ]\n",
    "\n",
    "            nn_model.fit(\n",
    "                X_train_processed, y_train,\n",
    "                validation_data=(X_val_processed, y_val),\n",
    "                epochs=200,\n",
    "                batch_size=32,\n",
    "                callbacks=callbacks,\n",
    "                class_weight=class_weights,\n",
    "                verbose=1\n",
    "            )\n",
    "            trained_models.append(nn_model)\n",
    "            self.evaluate_model(nn_model, X_test_processed, y_test, \"Neural Network\")\n",
    "\n",
    "            return trained_models\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in train_models: {str(e)}\")\n",
    "            logging.error(f\"Error details - X_train shape: {X_train.shape if hasattr(X_train, 'shape') else 'No shape'}\")\n",
    "            logging.error(f\"Stack trace: {traceback.format_exc()}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2R1yLqt6gNd3",
    "outputId": "7a605d6f-9a7c-4616-b928-e40c767376bc"
   },
   "outputs": [],
   "source": [
    "# Cell 8: Game Predictor Class\n",
    "# This class handles making predictions using the trained models.\n",
    "\n",
    "class GamePredictor:\n",
    "    def __init__(self, models, scaler, processor):\n",
    "        self.baseline_model, self.xgb_model, self.lgb_model, self.nn_model = models\n",
    "        self.scaler = scaler\n",
    "        self.processor = processor\n",
    "        self.config = NotebookConfig()\n",
    "\n",
    "        # Cache team data\n",
    "        self.nba_teams = teams.get_teams()\n",
    "        self.team_abbrev_to_id = {team['abbreviation']: team['id'] for team in self.nba_teams}\n",
    "\n",
    "        # Define core feature columns\n",
    "        self.core_features = [\n",
    "            'FG_PCT', 'FG3_PCT', 'FT_PCT', 'AST',\n",
    "            'REB', 'TOV', 'PLUS_MINUS', 'pts_per_min',\n",
    "            'ast_to_tov', 'fg_efficiency'\n",
    "        ]\n",
    "\n",
    "    def prepare_sample_game_data(self, team_abbrev, opponent_abbrev, game_date=None, is_home=True):\n",
    "        \"\"\"Create properly formatted sample game data.\"\"\"\n",
    "        if game_date is None:\n",
    "            game_date = datetime.now()\n",
    "\n",
    "        # Get team IDs\n",
    "        team_id = self.team_abbrev_to_id.get(team_abbrev)\n",
    "        opponent_id = self.team_abbrev_to_id.get(opponent_abbrev)\n",
    "\n",
    "        if not team_id or not opponent_id:\n",
    "            raise ValueError(f\"Invalid team abbreviation: {team_abbrev if not team_id else opponent_abbrev}\")\n",
    "\n",
    "        # Create matchup string\n",
    "        matchup = f\"{team_abbrev} vs. {opponent_abbrev}\" if is_home else f\"{team_abbrev} @ {opponent_abbrev}\"\n",
    "\n",
    "        # Create sample game data with all required fields\n",
    "        sample_game = pd.DataFrame({\n",
    "            'GAME_DATE': [game_date],\n",
    "            'TEAM_ID': [team_id],\n",
    "            'TEAM_ABBREVIATION': [team_abbrev],\n",
    "            'TEAM_NAME': [next((team['full_name'] for team in self.nba_teams if team['id'] == team_id), '')],\n",
    "            'OPPONENT_TEAM_ID': [opponent_id],\n",
    "            'OPPONENT_ABBREV': [opponent_abbrev],\n",
    "            'MATCHUP': [matchup],\n",
    "            'WL': ['W'],  # Placeholder\n",
    "            'MIN': [240],  # Standard game length\n",
    "            'PTS': [100],  # Placeholder stats\n",
    "            'FGM': [40],\n",
    "            'FGA': [80],\n",
    "            'FG_PCT': [0.500],\n",
    "            'FG3M': [10],\n",
    "            'FG3A': [25],\n",
    "            'FG3_PCT': [0.400],\n",
    "            'FTM': [10],\n",
    "            'FTA': [15],\n",
    "            'FT_PCT': [0.667],\n",
    "            'OREB': [10],\n",
    "            'DREB': [30],\n",
    "            'REB': [40],\n",
    "            'AST': [25],\n",
    "            'STL': [8],\n",
    "            'BLK': [5],\n",
    "            'TOV': [12],\n",
    "            'PF': [20],\n",
    "            'PLUS_MINUS': [0]\n",
    "        })\n",
    "\n",
    "        return sample_game\n",
    "\n",
    "    def predict_game(self, team_abbrev, opponent_abbrev, game_date=None, is_home=True):\n",
    "        \"\"\"Make ensemble prediction for a single game.\"\"\"\n",
    "        try:\n",
    "            # Prepare properly formatted game data\n",
    "            game_data = self.prepare_sample_game_data(team_abbrev, opponent_abbrev, game_date, is_home)\n",
    "\n",
    "            # Process game data\n",
    "            processed_data = self.processor.preprocess_game_data(game_data)\n",
    "            if processed_data.empty:\n",
    "                raise ValueError(\"No valid processed data\")\n",
    "\n",
    "            # Calculate derived features\n",
    "            processed_data['pts_per_min'] = processed_data['PTS'] / processed_data['MIN']\n",
    "            processed_data['ast_to_tov'] = processed_data['AST'] / (processed_data['TOV'] + 1)\n",
    "            processed_data['fg_efficiency'] = processed_data['FG_PCT'] * processed_data['FGA']\n",
    "\n",
    "            # Select only the core features used in training\n",
    "            X = processed_data[self.core_features]\n",
    "\n",
    "            # Scale features\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "\n",
    "            # Get predictions from each model\n",
    "            baseline_pred = self.baseline_model.predict_proba(X_scaled)[:, 1]\n",
    "            xgb_pred = self.xgb_model.predict_proba(X_scaled)[:, 1]\n",
    "            lgb_pred = self.lgb_model.predict_proba(X_scaled)[:, 1]\n",
    "            nn_pred = self.nn_model.predict(X_scaled).ravel()\n",
    "\n",
    "            # Ensemble prediction (weighted average)\n",
    "            weights = [0.2, 0.3, 0.3, 0.2]  # Adjustable weights for each model\n",
    "            ensemble_pred = (\n",
    "                weights[0] * baseline_pred +\n",
    "                weights[1] * xgb_pred +\n",
    "                weights[2] * lgb_pred +\n",
    "                weights[3] * nn_pred\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'prediction': float(ensemble_pred[0]),\n",
    "                'confidence': float(np.abs(ensemble_pred[0] - 0.5) * 2),\n",
    "                'model_predictions': {\n",
    "                    'baseline': float(baseline_pred[0]),\n",
    "                    'xgboost': float(xgb_pred[0]),\n",
    "                    'lightgbm': float(lgb_pred[0]),\n",
    "                    'neural_network': float(nn_pred[0])\n",
    "                }\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in predict_game: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "jWnSPs0hgPnW"
   },
   "outputs": [],
   "source": [
    "# Cell 9: FastAPI Setup\n",
    "# Setting up a local FastAPI application for making predictions via API.\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class GameData(BaseModel):\n",
    "    TEAM_ABBREVIATION: str\n",
    "    OPPONENT_ABBREV: str\n",
    "    GAME_DATE: str  # Date in 'YYYY-MM-DD' format\n",
    "    MATCHUP: str\n",
    "    WL: str  # Placeholder\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict_game_endpoint(game_data: GameData):\n",
    "    \"\"\"API endpoint for predicting a game's outcome.\"\"\"\n",
    "    try:\n",
    "        input_df = pd.DataFrame([game_data.dict()])\n",
    "        predictor = GamePredictor(models, scaler, processor)\n",
    "        prediction = predictor.predict_game(input_df)\n",
    "        return prediction\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870,
     "referenced_widgets": [
      "2c883cb0bd4d475da0daa7069b2188a4",
      "03cf8df14fb744e3b2ae79713ea63627",
      "d30e14ea32b2456db77278e02fe9219b",
      "fe729340ee374333845a9bee42d6d0b0",
      "899bee3331a3442192dc6569bd061ab2",
      "45b0391df2834c9f82b4785498db3c7b",
      "1080fcc23a9e414580ff0a178c063a73",
      "c37e85f658444b469114883ab6a1d2c4",
      "4fb5fdb222fb428ba8b74972ecc6bf50",
      "a7314aec0c15409189489fd6dab5d2dc",
      "cf6423da94a64aee8ec200f4312946f0",
      "138497201475493db193467240307980",
      "280c1aa78269409d84466829463776ef",
      "2f6cd8d7a2b545cc8299dc5a90dce241",
      "813fb70b2cfb4c6d814344252e8300be",
      "076a40bdc773402ca477b3bd4f254fe7",
      "ccb6de02b0b34a7c81f073c6ebf764eb",
      "d00f6198adf747c0901ac48136191501",
      "e9eb33218b4941b49e1fa94a60f4449d",
      "1cf883756eb34a0d88656b1765730100",
      "112262f55d6b42c9aa1f476223f6dde0",
      "b933c857ac5340feaca30058a624a269",
      "022c5e5586ff4e0ea7de93240c91c09c",
      "c78de212c4824e668e917402c4d3508a",
      "eacac327d2a84e90a7c9040ce0bdc635",
      "75d8df1af7d741038656ae7f42b7d0da"
     ]
    },
    "id": "eEOrito4gRvk",
    "outputId": "13701297-7123-4817-8db4-7708c62a7ff2"
   },
   "outputs": [],
   "source": [
    "# Cell 10: Interactive Interface\n",
    "# Creating an interactive widget interface for making predictions in the notebook.\n",
    "\n",
    "def get_valid_team_abbreviations():\n",
    "    \"\"\"Get list of valid NBA team abbreviations.\"\"\"\n",
    "    return [\n",
    "        'ATL', 'BOS', 'BKN', 'CHA', 'CHI', 'CLE', 'DAL', 'DEN', 'DET', 'GSW',\n",
    "        'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA', 'MIL', 'MIN', 'NOP', 'NYK',\n",
    "        'OKC', 'ORL', 'PHI', 'PHX', 'POR', 'SAC', 'SAS', 'TOR', 'UTA', 'WAS'\n",
    "    ]\n",
    "\n",
    "def validate_team_abbreviation(abbrev):\n",
    "    \"\"\"Validate team abbreviation.\"\"\"\n",
    "    valid_teams = get_valid_team_abbreviations()\n",
    "    if abbrev not in valid_teams:\n",
    "        raise ValueError(\n",
    "            f\"Invalid team abbreviation: {abbrev}\\n\"\n",
    "            f\"Valid abbreviations are: {', '.join(valid_teams)}\"\n",
    "        )\n",
    "    return True\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self):\n",
    "        self.config = NotebookConfig()\n",
    "        self.data_manager = DataManager()\n",
    "        self.processor = DataProcessor()\n",
    "        self.trainer = ModelTrainer()\n",
    "\n",
    "    def train_and_save_models(self):\n",
    "        \"\"\"Train and save all models.\"\"\"\n",
    "        try:\n",
    "            # Fetch and process data\n",
    "            logging.info(\"Fetching NBA data...\")\n",
    "            game_data = fetch_or_load_data()\n",
    "            if game_data.empty:\n",
    "                raise ValueError(\"No game data available.\")\n",
    "\n",
    "            # Preprocess data\n",
    "            logging.info(\"Preprocessing game data...\")\n",
    "            processed_data = self.processor.preprocess_game_data(game_data)\n",
    "\n",
    "            # Calculate core features\n",
    "            processed_data['pts_per_min'] = processed_data['PTS'] / processed_data['MIN']\n",
    "            processed_data['ast_to_tov'] = processed_data['AST'] / (processed_data['TOV'] + 1)\n",
    "            processed_data['fg_efficiency'] = processed_data['FG_PCT'] * processed_data['FGA']\n",
    "\n",
    "            # Select core features\n",
    "            core_features = [\n",
    "                'FG_PCT', 'FG3_PCT', 'FT_PCT', 'AST',\n",
    "                'REB', 'TOV', 'PLUS_MINUS', 'pts_per_min',\n",
    "                'ast_to_tov', 'fg_efficiency'\n",
    "            ]\n",
    "\n",
    "            X = processed_data[core_features]\n",
    "            y = processed_data['WIN']\n",
    "\n",
    "            # Split data\n",
    "            train_size = int(0.7 * len(X))\n",
    "            val_size = int(0.15 * len(X))\n",
    "\n",
    "            X_train = X[:train_size]\n",
    "            y_train = y[:train_size]\n",
    "            X_val = X[train_size:train_size + val_size]\n",
    "            y_val = y[train_size:train_size + val_size]\n",
    "            X_test = X[train_size + val_size:]\n",
    "            y_test = y[train_size + val_size:]\n",
    "\n",
    "            # Train models\n",
    "            logging.info(\"Training models...\")\n",
    "            models = self.trainer.train_models(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "            scaler = self.trainer.scaler\n",
    "\n",
    "            # Save models\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            self.save_models(models, scaler, timestamp)\n",
    "\n",
    "            return models, scaler\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in train_and_save_models: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save_models(self, models, scaler, timestamp):\n",
    "        \"\"\"Save all models and scaler.\"\"\"\n",
    "        try:\n",
    "            # Create models directory if it doesn't exist\n",
    "            os.makedirs(self.config.MODELS_DIR, exist_ok=True)\n",
    "\n",
    "            # Save individual models\n",
    "            logging.info(\"Saving baseline model...\")\n",
    "            joblib.dump(models[0], f'{self.config.MODELS_DIR}/baseline_model_{timestamp}.joblib')\n",
    "\n",
    "            logging.info(\"Saving XGBoost model...\")\n",
    "            models[1].save_model(f'{self.config.MODELS_DIR}/xgb_model_{timestamp}.json')\n",
    "\n",
    "            logging.info(\"Saving LightGBM model...\")\n",
    "            # Save LightGBM model directly using the booster\n",
    "            if hasattr(models[2], '_Booster'):\n",
    "                models[2]._Booster.save_model(f'{self.config.MODELS_DIR}/lgb_model_{timestamp}.txt')\n",
    "            else:\n",
    "                models[2].booster_.save_model(f'{self.config.MODELS_DIR}/lgb_model_{timestamp}.txt')\n",
    "\n",
    "            logging.info(\"Saving Neural Network model...\")\n",
    "            models[3].save(f'{self.config.MODELS_DIR}/nn_model_{timestamp}.keras')\n",
    "\n",
    "            logging.info(\"Saving scaler...\")\n",
    "            with open(f'{self.config.MODELS_DIR}/scaler_{timestamp}.pkl', 'wb') as f:\n",
    "                pickle.dump(scaler, f)\n",
    "\n",
    "            logging.info(f\"Models saved with timestamp: {timestamp}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving models: {str(e)}\")\n",
    "            logging.error(f\"Full error: {traceback.format_exc()}\")\n",
    "            raise\n",
    "\n",
    "    def load_most_recent_models(self):\n",
    "        \"\"\"Load the most recently saved models.\"\"\"\n",
    "        try:\n",
    "            models_dir = Path(self.config.MODELS_DIR)\n",
    "            if not models_dir.exists():\n",
    "                logging.info(f\"Models directory not found: {models_dir}\")\n",
    "                return\n",
    "\n",
    "            # Get all model files\n",
    "            model_files = list(models_dir.glob(\"baseline_model_*.joblib\"))\n",
    "            if not model_files:\n",
    "                logging.info(\"No model files found\")\n",
    "                return\n",
    "\n",
    "            # Extract timestamps and get the most recent one\n",
    "            timestamps = []\n",
    "            for file in model_files:\n",
    "                try:\n",
    "                    timestamp = file.stem.split('baseline_model_')[1]\n",
    "                    timestamps.append(timestamp)\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Couldn't parse timestamp from file: {file}\")\n",
    "                    continue\n",
    "\n",
    "            if not timestamps:\n",
    "                logging.info(\"No valid timestamps found\")\n",
    "                return\n",
    "\n",
    "            latest_timestamp = max(timestamps)\n",
    "            logging.info(f\"Found latest timestamp: {latest_timestamp}\")\n",
    "\n",
    "            # Construct full paths for all model files\n",
    "            baseline_path = models_dir / f\"baseline_model_{latest_timestamp}.joblib\"\n",
    "            xgb_path = models_dir / f\"xgb_model_{latest_timestamp}.json\"\n",
    "            lgb_path = models_dir / f\"lgb_model_{latest_timestamp}.txt\"\n",
    "            nn_path = models_dir / f\"nn_model_{latest_timestamp}.keras\"\n",
    "            scaler_path = models_dir / f\"scaler_{latest_timestamp}.pkl\"\n",
    "\n",
    "            # Verify all files exist\n",
    "            required_files = [baseline_path, xgb_path, lgb_path, nn_path, scaler_path]\n",
    "            for file in required_files:\n",
    "                if not file.exists():\n",
    "                    logging.error(f\"Required model file not found: {file}\")\n",
    "                    return\n",
    "\n",
    "            # Load models\n",
    "            models = []\n",
    "\n",
    "            # Load baseline model\n",
    "            logging.info(\"Loading baseline model...\")\n",
    "            models.append(joblib.load(baseline_path))\n",
    "\n",
    "            # Load XGBoost model\n",
    "            logging.info(\"Loading XGBoost model...\")\n",
    "            xgb_model = xgb.XGBClassifier()\n",
    "            xgb_model.load_model(str(xgb_path))\n",
    "            models.append(xgb_model)\n",
    "\n",
    "            # Load LightGBM model\n",
    "            logging.info(\"Loading LightGBM model...\")\n",
    "            booster = lgb.Booster(model_file=str(lgb_path))\n",
    "            lgb_model = lgb.LGBMClassifier(n_estimators=100)  # Initialize with default params\n",
    "            lgb_model._Booster = booster  # Set the booster\n",
    "            lgb_model._n_features = booster.num_feature()  # Set number of features\n",
    "            lgb_model._n_classes = 2  # Binary classification\n",
    "            lgb_model._classes = np.array([0, 1])  # Binary classes\n",
    "            models.append(lgb_model)\n",
    "\n",
    "            # Load Neural Network model\n",
    "            logging.info(\"Loading Neural Network model...\")\n",
    "            nn_model = tf.keras.models.load_model(str(nn_path))\n",
    "            models.append(nn_model)\n",
    "\n",
    "            # Load scaler\n",
    "            logging.info(\"Loading scaler...\")\n",
    "            with open(scaler_path, 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "\n",
    "            self.models = models\n",
    "            self.scaler = scaler\n",
    "            logging.info(f\"Successfully loaded all models from timestamp: {latest_timestamp}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading models: {str(e)}\")\n",
    "            logging.error(f\"Full error: {traceback.format_exc()}\")\n",
    "            self.models = None\n",
    "            self.scaler = None\n",
    "\n",
    "    def predict_with_models(self, X):\n",
    "        \"\"\"Make predictions with all models.\"\"\"\n",
    "        if self.models is None or self.scaler is None:\n",
    "            raise ValueError(\"Models not loaded\")\n",
    "\n",
    "        # Scale the input data\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "\n",
    "        predictions = []\n",
    "        for i, model in enumerate(self.models):\n",
    "            try:\n",
    "                if isinstance(model, lgb.LGBMClassifier):\n",
    "                    pred = model.predict_proba(X_scaled)[:, 1]\n",
    "                elif hasattr(model, 'predict_proba'):\n",
    "                    pred = model.predict_proba(X_scaled)[:, 1]\n",
    "                else:\n",
    "                    pred = model.predict(X_scaled).ravel()\n",
    "                predictions.append(pred)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error predicting with model {i}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        return np.mean(predictions, axis=0)\n",
    "\n",
    "def create_prediction_interface():\n",
    "    \"\"\"Create and display the prediction interface.\"\"\"\n",
    "    # Create widgets\n",
    "    home_team = widgets.Text(description='Home Team:', value='')\n",
    "    away_team = widgets.Text(description='Away Team:', value='')\n",
    "    train_button = widgets.Button(description='Train Models')\n",
    "    predict_button = widgets.Button(description='Predict')\n",
    "    output = widgets.Output()\n",
    "    status_label = widgets.HTML(value=\"\")  # Add status label\n",
    "\n",
    "    # Initialize prediction interface\n",
    "    interface = PredictionInterface()\n",
    "\n",
    "    def update_status():\n",
    "        \"\"\"Update status label based on model availability.\"\"\"\n",
    "        if interface.models is not None and interface.scaler is not None:\n",
    "            status_label.value = '<p style=\"color: green;\">Models loaded and ready</p>'\n",
    "        else:\n",
    "            status_label.value = '<p style=\"color: red;\">No models loaded - please train first</p>'\n",
    "\n",
    "    update_status()  # Initial status update\n",
    "\n",
    "    def on_train_button_clicked(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            try:\n",
    "                model_manager = ModelManager()\n",
    "                print(\"Training models... This may take a few minutes.\")\n",
    "                interface.models, interface.scaler = model_manager.train_and_save_models()\n",
    "                print(\"Models trained and saved successfully!\")\n",
    "                update_status()\n",
    "            except Exception as e:\n",
    "                print(f\"Error training models: {str(e)}\")\n",
    "                logging.error(f\"Training error: {traceback.format_exc()}\")\n",
    "\n",
    "    def on_predict_button_clicked(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            try:\n",
    "                if interface.models is None or interface.scaler is None:\n",
    "                    print(\"Please train models first!\")\n",
    "                    return\n",
    "\n",
    "                if not home_team.value or not away_team.value:\n",
    "                    print(\"Please enter both home and away teams!\")\n",
    "                    return\n",
    "\n",
    "                # Validate team abbreviations\n",
    "                home_abbrev = home_team.value.upper()\n",
    "                away_abbrev = away_team.value.upper()\n",
    "\n",
    "                try:\n",
    "                    validate_team_abbreviation(home_abbrev)\n",
    "                    validate_team_abbreviation(away_abbrev)\n",
    "                except ValueError as e:\n",
    "                    print(str(e))\n",
    "                    return\n",
    "\n",
    "                predictor = GamePredictor(interface.models, interface.scaler, interface.processor)\n",
    "                prediction = predictor.predict_game(\n",
    "                    team_abbrev=home_abbrev,\n",
    "                    opponent_abbrev=away_abbrev,\n",
    "                    is_home=True\n",
    "                )\n",
    "\n",
    "                print(f\"\\nPrediction for {home_abbrev} vs {away_abbrev}:\")\n",
    "                print(f\"Win Probability for {home_abbrev}: {prediction['prediction']:.2%}\")\n",
    "                print(f\"Confidence: {prediction['confidence']:.2%}\")\n",
    "                print(\"\\nModel Predictions:\")\n",
    "                for model, pred in prediction['model_predictions'].items():\n",
    "                    print(f\"{model.capitalize()}: {pred:.2%}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during prediction: {str(e)}\")\n",
    "                logging.error(f\"Prediction error: {traceback.format_exc()}\")\n",
    "\n",
    "    # Connect buttons to callbacks\n",
    "    train_button.on_click(on_train_button_clicked)\n",
    "    predict_button.on_click(on_predict_button_clicked)\n",
    "\n",
    "    # Display widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>NBA Game Prediction</h3>\"),\n",
    "        status_label,\n",
    "        widgets.HBox([home_team, away_team]),\n",
    "        widgets.HBox([train_button, predict_button]),\n",
    "        output\n",
    "    ]))\n",
    "\n",
    "# Create the interface when running this cell\n",
    "create_prediction_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znuXhUW6gUHL",
    "outputId": "b6aec8a6-6122-4c78-a5d3-8dcf790b2a31"
   },
   "outputs": [],
   "source": [
    "# Cell 11: Initialization of Components\n",
    "\n",
    "# Initialize components\n",
    "logging.info(\"Initializing components...\")\n",
    "data_manager = DataManager()\n",
    "processor = DataProcessor()\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# Fetch data\n",
    "logging.info(\"Fetching NBA data...\")\n",
    "game_data = fetch_or_load_data()\n",
    "if game_data.empty:\n",
    "    logging.error(\"No game data available.\")\n",
    "else:\n",
    "    # Preprocess data\n",
    "    logging.info(\"Preprocessing game data...\")\n",
    "    features = processor.preprocess_game_data(game_data)\n",
    "    features = processor.prepare_features(features)\n",
    "\n",
    "    # Split the dataset into features and target\n",
    "    X = features.drop(['GAME_DATE', 'WIN'], axis=1)\n",
    "    y = features['WIN']\n",
    "\n",
    "    # Split data into training, validation, and test sets\n",
    "    train_size = int(0.7 * len(features))\n",
    "    val_size = int(0.15 * len(features))\n",
    "\n",
    "    X_train, y_train = X[:train_size], y[:train_size]\n",
    "    X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "    X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "\n",
    "    # Train models\n",
    "    logging.info(\"Training models...\")\n",
    "    models = trainer.train_models(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    scaler = trainer.scaler\n",
    "\n",
    "    # Save models and scaler\n",
    "    logging.info(\"Saving models...\")\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    joblib.dump(models[0], f'{NotebookConfig.MODELS_DIR}/baseline_model_{timestamp}.joblib')\n",
    "    models[1].save_model(f'{NotebookConfig.MODELS_DIR}/xgb_model_{timestamp}.json')\n",
    "    models[2].booster_.save_model(f'{NotebookConfig.MODELS_DIR}/lgb_model_{timestamp}.txt')\n",
    "    models[3].save(f'{NotebookConfig.MODELS_DIR}/nn_model_{timestamp}.h5')\n",
    "    with open(f'{NotebookConfig.MODELS_DIR}/scaler_{timestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    logging.info(\"Initialization and training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkUkII3OgWHE",
    "outputId": "8af52761-e505-4bcb-c530-dcd5d58c8aba"
   },
   "outputs": [],
   "source": [
    "# Cell 12: Game Schedule and Predictions Formatter\n",
    "# This class formats predictions for upcoming games.\n",
    "\n",
    "class NBAGamePredictor:\n",
    "    def __init__(self, models, scaler, processor):\n",
    "        self.predictor = GamePredictor(models, scaler, processor)\n",
    "        self.config = NotebookConfig()\n",
    "\n",
    "    def fetch_upcoming_games(self):\n",
    "        \"\"\"Fetch upcoming NBA games and odds.\"\"\"\n",
    "        params = {\n",
    "            'apiKey': self.config.ODDS_API_KEY,\n",
    "            'regions': self.config.REGIONS,\n",
    "            'markets': 'h2h',\n",
    "            'oddsFormat': self.config.ODDS_FORMAT\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.config.ODDS_API_URL, params=params)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                logging.error(f\"Error fetching odds: {response.status_code}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching upcoming games: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def format_predictions(self):\n",
    "        \"\"\"Format predictions for upcoming games.\"\"\"\n",
    "        games = self.fetch_upcoming_games()\n",
    "        if not games:\n",
    "            return \"Unable to fetch upcoming games\"\n",
    "\n",
    "        output = \"Model Predictions:\\n\"\n",
    "        for game in games:\n",
    "            home_team = game['home_team']\n",
    "            away_team = game['away_team']\n",
    "\n",
    "            # Get win probabilities\n",
    "            home_pred = self.predictor.predict_game(pd.DataFrame([{\n",
    "                'TEAM_ABBREVIATION': home_team,\n",
    "                'OPPONENT_ABBREV': away_team,\n",
    "                'GAME_DATE': datetime.now().strftime('%Y-%m-%d'),\n",
    "                'MATCHUP': f\"{home_team} vs. {away_team}\",\n",
    "                'WL': 'W'  # Placeholder\n",
    "            }]))['prediction']\n",
    "\n",
    "            output += f\"{home_team} vs {away_team} - {home_team} Win Probability: {home_pred:.2%}\\n\"\n",
    "\n",
    "        return output\n",
    "\n",
    "# Test NBAGamePredictor class\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Assume models, scaler, and processor are already initialized\n",
    "        nba_predictor = NBAGamePredictor(models, scaler, processor)\n",
    "        predictions_output = nba_predictor.format_predictions()\n",
    "        print(predictions_output)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error running predictions: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "022c5e5586ff4e0ea7de93240c91c09c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "03cf8df14fb744e3b2ae79713ea63627": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c37e85f658444b469114883ab6a1d2c4",
      "placeholder": "",
      "style": "IPY_MODEL_4fb5fdb222fb428ba8b74972ecc6bf50",
      "value": "<h3>NBA Game Prediction</h3>"
     }
    },
    "076a40bdc773402ca477b3bd4f254fe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Predict",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_c78de212c4824e668e917402c4d3508a",
      "style": "IPY_MODEL_eacac327d2a84e90a7c9040ce0bdc635",
      "tooltip": ""
     }
    },
    "1080fcc23a9e414580ff0a178c063a73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "112262f55d6b42c9aa1f476223f6dde0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "138497201475493db193467240307980": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Home Team:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_d00f6198adf747c0901ac48136191501",
      "placeholder": "",
      "style": "IPY_MODEL_e9eb33218b4941b49e1fa94a60f4449d",
      "value": "BKN"
     }
    },
    "1cf883756eb34a0d88656b1765730100": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "280c1aa78269409d84466829463776ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Away Team:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_1cf883756eb34a0d88656b1765730100",
      "placeholder": "",
      "style": "IPY_MODEL_112262f55d6b42c9aa1f476223f6dde0",
      "value": "BOS"
     }
    },
    "2c883cb0bd4d475da0daa7069b2188a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03cf8df14fb744e3b2ae79713ea63627",
       "IPY_MODEL_d30e14ea32b2456db77278e02fe9219b",
       "IPY_MODEL_fe729340ee374333845a9bee42d6d0b0",
       "IPY_MODEL_899bee3331a3442192dc6569bd061ab2",
       "IPY_MODEL_45b0391df2834c9f82b4785498db3c7b"
      ],
      "layout": "IPY_MODEL_1080fcc23a9e414580ff0a178c063a73"
     }
    },
    "2f6cd8d7a2b545cc8299dc5a90dce241": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45b0391df2834c9f82b4785498db3c7b": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_75d8df1af7d741038656ae7f42b7d0da",
      "msg_id": "",
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,535 - INFO - Initial data shape: (1, 28)\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,537 - INFO - Initial columns: Index(['GAME_DATE', 'TEAM_ID', 'TEAM_ABBREVIATION', 'TEAM_NAME',\n",
         "       'OPPONENT_TEAM_ID', 'OPPONENT_ABBREV', 'MATCHUP', 'WL', 'MIN', 'PTS',\n",
         "       'FGM', 'FGA', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', 'FTM', 'FTA',\n",
         "       'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TOV', 'PF',\n",
         "       'PLUS_MINUS'],\n",
         "      dtype='object')\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,542 - INFO - Created WIN column from WL\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,545 - INFO - After date conversion shape: (1, 29)\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,551 - INFO - After team abbreviation mapping shape: (1, 29)\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,579 - INFO - After dropping missing opponents shape: (1, 34)\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,607 - INFO - Adding fatigue features...\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,625 - INFO - Adding advanced features...\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,653 - INFO - Adding rolling stats...\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,781 - INFO - Adding head-to-head features...\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,790 - INFO - Adding streak features...\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,798 - INFO - Adding time-based features...\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,799 - INFO - Starting add_time_based_features\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,804 - INFO - Initial shape: (1, 114)\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,813 - INFO - Final shape after time-based features: (1, 117)\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "2024-11-13 19:50:51,831 - INFO - Final shape: (1, 117)\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "\n",
         "Prediction for BKN vs BOS:\n",
         "Win Probability for BKN: 33.95%\n",
         "Confidence: 32.10%\n",
         "\n",
         "Model Predictions:\n",
         "Baseline: 47.92%\n",
         "Xgboost: 9.04%\n",
         "Lightgbm: 33.55%\n",
         "Neural_network: 57.95%\n"
        ]
       }
      ]
     }
    },
    "4fb5fdb222fb428ba8b74972ecc6bf50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75d8df1af7d741038656ae7f42b7d0da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "813fb70b2cfb4c6d814344252e8300be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Train Models",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_b933c857ac5340feaca30058a624a269",
      "style": "IPY_MODEL_022c5e5586ff4e0ea7de93240c91c09c",
      "tooltip": ""
     }
    },
    "899bee3331a3442192dc6569bd061ab2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_813fb70b2cfb4c6d814344252e8300be",
       "IPY_MODEL_076a40bdc773402ca477b3bd4f254fe7"
      ],
      "layout": "IPY_MODEL_ccb6de02b0b34a7c81f073c6ebf764eb"
     }
    },
    "a7314aec0c15409189489fd6dab5d2dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b933c857ac5340feaca30058a624a269": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c37e85f658444b469114883ab6a1d2c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c78de212c4824e668e917402c4d3508a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccb6de02b0b34a7c81f073c6ebf764eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf6423da94a64aee8ec200f4312946f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d00f6198adf747c0901ac48136191501": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d30e14ea32b2456db77278e02fe9219b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7314aec0c15409189489fd6dab5d2dc",
      "placeholder": "",
      "style": "IPY_MODEL_cf6423da94a64aee8ec200f4312946f0",
      "value": "<p style=\"color: green;\">Models loaded and ready</p>"
     }
    },
    "e9eb33218b4941b49e1fa94a60f4449d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eacac327d2a84e90a7c9040ce0bdc635": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "fe729340ee374333845a9bee42d6d0b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_138497201475493db193467240307980",
       "IPY_MODEL_280c1aa78269409d84466829463776ef"
      ],
      "layout": "IPY_MODEL_2f6cd8d7a2b545cc8299dc5a90dce241"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
